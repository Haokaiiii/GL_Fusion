# Model Configuration for GL-Fusion (GNN+LLM)

# Paths
data:
  raw_dir: "data/raw"
  processed_dir: "data/processed"
  llm_prepared_dir: "data/prepared_for_llm"
  train_val_split_task1: "data/processed/train_val_split_task1.pkl"
  test_split_task1: "data/processed/test_split_task1.pkl"
  train_val_split_task2: "data/processed/train_val_split_task2.pkl"
  test_split_task2: "data/processed/test_split_task2.pkl"
  graph_data: "data/processed/graph_data.pt"
  node_mapping: "data/processed/node_mapping.json"
  max_samples: 3000  # Reduced further to avoid OOM
  sequence_length: 128  # Reduced sequence length to save memory
  edge_method: "spatial_proximity"  # Method for creating edges in the graph
  max_edge_distance: 2.0  # Maximum distance for edge creation (in km or grid cells)
  num_workers: 4  # Dataloader workers
  task_id: 1  # Default task ID (1 or 2)
  poi_categories: data/raw/POI_datacategories.csv

# GNN Parameters
gnn:
  model_type: "GAT"  # Graph Attention Network
  input_dim: 128     # Input dimension after projection
  hidden_dim: 64     # Reduced to save memory
  num_layers: 1      # Reduced to save memory
  heads: 2           # Reduced number of attention heads
  dropout: 0.2
  use_edge_features: false
  edge_creation_method: "spatial_proximity"
  max_edge_distance: 2

# LLM Parameters
llm:
  model_name: "Qwen/Qwen2.5-7B-Instruct"
  local_model_path: "/g/data/hn98/models/Qwen2.5-7B-Instruct-1M"
  device: "auto"
  use_lora: true
  lora_config:
    r: 64                    # Increased to 64 for Qwen2.5's complexity
    lora_alpha: 128          # Set to 2*r for optimal scaling
    lora_dropout: 0.1        # Keep at 0.1 for good regularization
    bias: "lora_only"        # Enable LoRA bias adaptation for Qwen2.5
    # Optimized for Qwen2.5 architecture with GQA and SwiGLU
    target_modules: [
      "q_proj", "k_proj", "v_proj", "o_proj",  # Attention projections
      "gate_proj", "up_proj", "down_proj",     # SwiGLU MLP components
      "embed_tokens", "lm_head"                # Input/output embeddings
    ]
    # Advanced LoRA settings for Qwen2.5
    task_type: "CAUSAL_LM"
    inference_mode: false
    modules_to_save: ["embed_tokens", "lm_head"]  # Save embedding adaptations
  hidden_size: 3584  # Qwen2.5-7B hidden size
  sequence_length: 2048  # Increased from 512 to better utilize context
  node_text_sequence_length: 128  # Increased for better node descriptions
  quantization:
    load_in_8bit: false
    load_in_4bit: false  # Set to true for 4-bit quantization if needed
    bnb_4bit_quant_type: "nf4"  # Quantization type (e.g., nf4)
    bnb_4bit_compute_dtype: "bfloat16"  # Compute dtype (e.g., bfloat16)
    bnb_4bit_use_double_quant: true

# Structure-Aware Transformer Parameters
structure_aware:
  num_layers: 1
  dropout: 0.1

# Fusion Parameters
fusion:
  mechanism: "cross_attention"
  hidden_dim: 64    # Reduced to save memory
  num_heads: 2      # Reduced to save memory
  dropout: 0.1

# Twin Predictor Parameters
twin_predictor:
  use_twin_predictor: true
  gnn_weight: 0.3
  auxiliary_loss_weight: 0.1

# Training Parameters
training:
  batch_size: 1     # Start with 1 due to increased sequence length
  num_epochs: 50     # Increased for production training
  gradient_clip_val: 1.0
  early_stopping_patience: 7  # Increased patience for Qwen2.5
  validation_interval: 1
  ddp_static_graph: true
  gradient_accumulation_steps: 8  # Increased to simulate larger batch
  use_amp: true
  learning_rate: 2e-4  # Higher LR works better with larger LoRA rank
  weight_decay: 0.01
  scheduler:
    type: "cosine"   # Cosine scheduler works better for Qwen models
    num_warmup_steps: 2000  # Longer warmup for stability
    warmup_ratio: 0.05  # 5% warmup
    patience: 3
    factor: 0.5
  seed: 42
  train_data_subsample_ratio: 0.5  # 50% of dataset (can be overridden by DATA_SUBSAMPLE_RATIO env var)
  # Qwen2.5 specific optimizations
  max_grad_norm: 1.0  # Explicit gradient norm clipping
  dataloader_num_workers: 4  # Parallel data loading

# Evaluation Parameters
evaluation:
  metrics: ["mse", "geo_bleu", "dtw"] 
  prediction_horizon: 30
  batch_size: 4

# Hardware
hardware:
  num_workers: 0  # Disable multiprocessing for stability

# Logging
logging:
  log_dir: "results/logs"
  checkpoint_dir: "results/checkpoints"
  prediction_dir: "results/predictions"
  team_name: "GLFusion"
  log_level: INFO
  log_file: results/logs/training.log
  tensorboard_dir: results/logs/tensorboard
  use_tensorboard: false
  wandb:
    use_wandb: false
    project: "gl-fusion-humob"
    run_name: "qwen-gat-fusion-task1"
  save_steps: 1000
  prediction_horizon: 1 