# Model Configuration for GL-Fusion (GNN+LLM)

# Paths
data:
  raw_dir: "data/raw"
  processed_dir: "data/processed"
  llm_prepared_dir: "data/prepared_for_llm"
  train_val_split_task1: "data/processed/train_val_split_task1.pkl"
  test_split_task1: "data/processed/test_split_task1.pkl"
  train_val_split_task2: "data/processed/train_val_split_task2.pkl"
  test_split_task2: "data/processed/test_split_task2.pkl"
  graph_data: "data/processed/graph_data.pt"
  node_mapping: "data/processed/node_mapping.json"
  max_samples: 3000  # Reduced further to avoid OOM
  sequence_length: 128  # Reduced sequence length to save memory
  edge_method: "spatial_proximity"  # Method for creating edges in the graph
  max_edge_distance: 2.0  # Maximum distance for edge creation (in km or grid cells)
  num_workers: 4  # Dataloader workers
  task_id: 1  # Default task ID (1 or 2)
  poi_categories: data/raw/POI_datacategories.csv

# GNN Parameters
gnn:
  model_type: "GAT"  # Graph Attention Network
  input_dim: 128     # Input dimension after projection
  hidden_dim: 64     # Reduced to save memory
  num_layers: 1      # Reduced to save memory
  heads: 2           # Reduced number of attention heads
  dropout: 0.2
  use_edge_features: false
  edge_creation_method: "spatial_proximity"
  max_edge_distance: 2

# LLM Parameters
llm:
  model_name: "Qwen/Qwen2.5-7B-Instruct"
  local_model_path: "/g/data/hn98/models/Qwen2.5-7B-Instruct-1M"
  device: "auto"
  use_lora: true
  lora_config:
    r: 8
    lora_alpha: 16
    lora_dropout: 0.05
    bias: "none"
    target_modules: ["q_proj", "v_proj"]
  hidden_size: 3584  # Example hidden size for LLM
  sequence_length: 512  # Max sequence length for LLM input
  node_text_sequence_length: 64  # Max length for node text descriptions
  quantization:
    load_in_8bit: false
    load_in_4bit: false  # Set to true for 4-bit quantization if needed
    bnb_4bit_quant_type: "nf4"  # Quantization type (e.g., nf4)
    bnb_4bit_compute_dtype: "bfloat16"  # Compute dtype (e.g., bfloat16)
    bnb_4bit_use_double_quant: true

# Structure-Aware Transformer Parameters
structure_aware:
  num_layers: 1
  dropout: 0.1

# Fusion Parameters
fusion:
  mechanism: "cross_attention"
  hidden_dim: 64    # Reduced to save memory
  num_heads: 2      # Reduced to save memory
  dropout: 0.1

# Twin Predictor Parameters
twin_predictor:
  use_twin_predictor: true
  gnn_weight: 0.3
  auxiliary_loss_weight: 0.1

# Training Parameters
training:
  batch_size: 1     # Start with 1 for stability
  num_epochs: 50     # Increased for production training
  gradient_clip_val: 1.0
  early_stopping_patience: 5  # Increased for production training
  validation_interval: 1
  ddp_static_graph: true
  gradient_accumulation_steps: 4  # Reasonable for single GPU
  use_amp: true
  learning_rate: 1e-4  # Slightly higher for faster convergence
  weight_decay: 0.01
  scheduler:
    type: "linear"
    num_warmup_steps: 500  # Increased for longer training
    warmup_ratio: 0.1
    patience: 2
    factor: 0.5
  seed: 42
  train_data_subsample_ratio: 1.0  # Default to full dataset

# Evaluation Parameters
evaluation:
  metrics: ["mse", "geo_bleu", "dtw"] 
  prediction_horizon: 30
  batch_size: 4

# Hardware
hardware:
  num_workers: 0  # Disable multiprocessing for stability

# Logging
logging:
  log_dir: "results/logs"
  checkpoint_dir: "results/checkpoints"
  prediction_dir: "results/predictions"
  team_name: "GLFusion"
  log_level: INFO
  log_file: results/logs/training.log
  tensorboard_dir: results/logs/tensorboard
  use_tensorboard: false
  wandb:
    use_wandb: false
    project: "gl-fusion-humob"
    run_name: "qwen-gat-fusion-task1"
  save_steps: 1000
  prediction_horizon: 1 